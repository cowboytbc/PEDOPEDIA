"""
Download ALL Epstein-related documents from CourtListener

This script will:
1. Search CourtListener for all Epstein-related dockets
2. Download every document from every case
3. Extract text and build a comprehensive database

WARNING: This could download THOUSANDS of files and take hours!
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
import time
import os
from pathlib import Path
import json
import re

class ComprehensiveEpsteinDownloader:
    def __init__(self):
        self.download_dir = Path("epstein_documents/pdfs").absolute()
        self.download_dir.mkdir(parents=True, exist_ok=True)
        self.downloaded_count = 0
        self.failed_count = 0
        self.docket_list = []
        
    def setup_browser(self):
        """Setup Chrome browser with download preferences"""
        chrome_options = Options()
        chrome_options.add_experimental_option('prefs', {
            "download.default_directory": str(self.download_dir),
            "download.prompt_for_download": False,
            "download.directory_upgrade": True,
            "plugins.always_open_pdf_externally": True
        })
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.wait = WebDriverWait(self.driver, 10)
        
    def log(self, message):
        """Print and log message"""
        print(message)
        
    def check_for_captcha(self):
        """Check if CAPTCHA is present and pause if needed"""
        try:
            # Check for common CAPTCHA indicators
            captcha_indicators = [
                "recaptcha",
                "captcha",
                "verify you are human",
                "confirm you are human",
                "hcaptcha"
            ]
            
            page_source = self.driver.page_source.lower()
            
            for indicator in captcha_indicators:
                if indicator in page_source:
                    self.log("\n" + "="*70)
                    self.log("ü§ñ CAPTCHA DETECTED!")
                    self.log("="*70)
                    self.log("‚è∏Ô∏è  PAUSING - Please solve the CAPTCHA in the browser window")
                    self.log("‚è∏Ô∏è  Press ENTER when you've completed the CAPTCHA...")
                    self.log("="*70)
                    input()  # Wait for user to press Enter
                    self.log("‚úÖ Resuming downloads...\n")
                    return True
                    
            return False
            
        except Exception as e:
            self.log(f"‚ö†Ô∏è Error checking for CAPTCHA: {e}")
            return False
        
    def find_all_epstein_dockets(self):
        """Search CourtListener for ALL Epstein-related cases"""
        self.log("\n" + "="*70)
        self.log("üîç SEARCHING FOR JEFFREY EPSTEIN SEX TRAFFICKING CASES")
        self.log("="*70)
        
        search_terms = [
            "jeffrey epstein sex",
            "jeffrey epstein minor",
            "jeffrey epstein victim",
            "ghislaine maxwell",
            "epstein abuse",
            "epstein trafficking",
            "giuffre epstein",
            "jane doe epstein abuse"
        ]
        
        all_dockets = set()
        
        # Blacklist - exclude these people who have nothing to do with Jeffrey Epstein
        blacklist = [
            'ronald epstein',
            'matthew epstein',
            'hayden epstein',
            'melissa epstein',
            'jeffrey m. epstein jr',
            'jeffrey s epstein',
            'bankruptcy'
        ]
        
        for search_term in search_terms:
            self.log(f"\nüîé Searching: {search_term}")
            url = f"https://www.courtlistener.com/?q={search_term.replace(' ', '+')}&type=r&order_by=score+desc"
            
            try:
                self.driver.get(url)
                time.sleep(3)
                
                # Find all docket links
                links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/docket/']")
                
                for link in links:
                    href = link.get_attribute('href')
                    text = link.text.lower()
                    
                    # Skip if it's a blacklisted person/case
                    if any(blocked in text.lower() or blocked in href.lower() for blocked in blacklist):
                        continue
                        
                    if '/docket/' in href and href not in all_dockets:
                        all_dockets.add(href)
                        
                self.log(f"   Found {len(links)} potential dockets")
                
            except Exception as e:
                self.log(f"   ‚ö†Ô∏è Error searching: {e}")
                
        self.docket_list = list(all_dockets)
        self.log(f"\n‚úÖ TOTAL UNIQUE DOCKETS FOUND: {len(self.docket_list)}")
        return self.docket_list
        
    def download_docket(self, docket_url):
        """Download all documents from a specific docket"""
        self.log(f"\n{'='*70}")
        self.log(f"üìÇ Processing: {docket_url}")
        self.log(f"{'='*70}")
        
        try:
            self.driver.get(docket_url)
            time.sleep(2)
            
            # Check for CAPTCHA
            if self.check_for_captcha():
                return
            
            # Get case name
            try:
                case_name = self.driver.find_element(By.CSS_SELECTOR, "h1").text
                self.log(f"üìã Case: {case_name}")
            except:
                case_name = "Unknown Case"
            
            # Find all PDF links directly on the page
            pdf_links = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='.pdf'], a[href*='/pdf/']")
            self.log(f"üìÑ Found {len(pdf_links)} PDF documents")
            
            if len(pdf_links) == 0:
                self.log("   ‚ö†Ô∏è No PDFs found on this docket, skipping...")
                return
            
            for i, pdf_link in enumerate(pdf_links, 1):
                try:
                    pdf_url = pdf_link.get_attribute('href')
                    
                    # Get document number from URL
                    doc_match = re.search(r'/(\d+)/', pdf_url)
                    doc_num = doc_match.group(1) if doc_match else str(i)
                    
                    # Download the PDF
                    self.log(f"   [{i}/{len(pdf_links)}] Downloading document #{doc_num}...")
                    self.driver.get(pdf_url)
                    time.sleep(2)  # Wait for download
                    
                    self.downloaded_count += 1
                    
                except Exception as e:
                    self.log(f"   ‚ùå Failed to download PDF: {e}")
                    self.failed_count += 1
                    
        except Exception as e:
            self.log(f"‚ùå Failed to process docket: {e}")
            
    def download_all(self):
        """Main download orchestrator"""
        self.log("\n" + "="*70)
        self.log("üöÄ COMPREHENSIVE EPSTEIN DOCUMENT DOWNLOADER")
        self.log("="*70)
        self.log("‚ö†Ô∏è  WARNING: This will download THOUSANDS of files!")
        self.log("‚ö†Ô∏è  This may take HOURS to complete!")
        self.log("="*70)
        
        try:
            self.setup_browser()
            
            # Step 1: Find all dockets
            dockets = self.find_all_epstein_dockets()
            
            if not dockets:
                self.log("‚ùå No dockets found!")
                return
                
            # Step 2: Download from each docket
            for idx, docket_url in enumerate(dockets, 1):
                self.log(f"\n{'#'*70}")
                self.log(f"DOCKET {idx}/{len(dockets)}")
                self.log(f"{'#'*70}")
                self.download_docket(docket_url)
                
                # Save progress periodically
                if idx % 5 == 0:
                    self.save_progress(idx)
                    
        except Exception as e:
            self.log(f"‚ùå FATAL ERROR: {e}")
            
        finally:
            self.log("\n" + "="*70)
            self.log("üìä DOWNLOAD COMPLETE")
            self.log("="*70)
            self.log(f"‚úÖ Successfully downloaded: {self.downloaded_count} documents")
            self.log(f"‚ùå Failed: {self.failed_count} documents")
            self.log(f"üìÇ Total dockets processed: {len(self.docket_list)}")
            self.log(f"üíæ Files saved to: {self.download_dir}")
            self.log("="*70)
            
            if hasattr(self, 'driver'):
                self.driver.quit()
                
    def save_progress(self, current_docket):
        """Save download progress"""
        progress = {
            'downloaded': self.downloaded_count,
            'failed': self.failed_count,
            'current_docket': current_docket,
            'total_dockets': len(self.docket_list)
        }
        
        with open('download_progress.json', 'w') as f:
            json.dump(progress, f, indent=2)
            
if __name__ == "__main__":
    downloader = ComprehensiveEpsteinDownloader()
    downloader.download_all()
